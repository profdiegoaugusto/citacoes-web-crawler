#  üìú Cita√ß√µes Web Crawler

![Quotes To Scrape](img/website_quotes_scrape.png "Website Quotes to Scrape")

## Criando Um Novo Projeto

Antes de come√ßarmos nossa coleta de dados, voc√™ precisa configurar um novo projeto Scrapy; caso contr√°rio nada funcionar√°! 
Usando a ferramenta de *Terminal* do seu Sistema Operacional, entre na pasta que voc√™ vai guardar seu c√≥digo e digite o comando abaixo:

```python
scrapy startproject quotes_scrape_crawler
```

Este comando vai criar uma nova pasta com o seguinte conte√∫do:

```bash
quotes_scrape_crawler/
    
    scrapy.cfg                  # Arquivo de configura√ß√£o de implanta√ß√£o.

    quotes_scrape_crawler/      # M√≥dulo Python do Projeto: voc√™ vai programar seu rastreador usando os arquivos aqui dentro
        
        __init__.py

        items.py                # Arquivo de Defini√ß√£o:

        middlewares.py          # Middlewares do Projeto:

        pipelines.py            # Pipelines do Projeto: 

        settings.py             # Arquivo de Configura√ß√£o do Projeto:

        spiders/                # IMPORTANTE: diret√≥rio onde, daqui a pouco, voc√™ colocar√° seus coletores (Spiders)
            __init__.py
```

## Criando Um Novo Coletor (Spider)

**Spiders** s√£o classes que voc√™ define para que o Scrapy as use para extrair informa√ß√µes de um site ou de um grupo de sites. Elas devem ser subclasses da classe `Spider` e definem as solicita√ß√µes iniciais a serem feitas. Opcionalmente, os *Spiders* podem descrever como o coletor vai seguir os links nas p√°ginas e como ser√° feito o *parser* do conte√∫do da p√°gina baixada para extra√ß√£o dados.

O c√≥digo abaixo √© nosso coletor (*Spider*) do website [Quotes to Scrape](http://quotes.toscrape.com/), copie e cole o c√≥digo em um novo arquivo chamado `citacoes.py` dentro da pasta `quotes_scrape_crawler/spiders`.

```python
import scrapy

class CitacoesSpider(scrapy.Spider):
    
    name = 'citacoes'                                       # Nome do coletor (Spider)
    start_urls = [                                          # Listas de endere√ßos que ser√£o visitados
        'http://quotes.toscrape.com/page/1',
        'http://quotes.toscrape.com/page/2',
    ]

    def parse(self, response):

        pagina = response.url.split("/")[-2]                # Obt√©m o n√∫mero da p√°gina na URL
        nome_arquivo = 'cita√ß√µes-{}.html'.format(pagina)    # Formata o nome do arquivo html
        
        with open(nome_arquivo, 'wb') as escritor:          # Crie a abre o arquivo para escrita
            escritor.write(response.body)                   # Escreve o conte√∫do coletado no arquivo
```

Em seguida, digite o comando abaixo no *Terminal* para executar seu coletor (*Spider*)

```bash
scrapy crawl citacoes
```

O m√©todo `parse()` visita cada uma das URLs da lista `start_url`, faz a coleta dos documentos `HTML` referente a cada endere√ßo web (URL) e salva cada uma das p√°ginas nos arquivos `cita√ß√µes-1.html` e `cita√ß√µes-1.html`; dentro do diret√≥rio raiz do projeto.


### genspider

√â poss√≠vel criar um novo *Spider* usando a ferramenta de linha de comando do *Terminal*, para isso, usaremos a sintaxe abaixo:

```bash
scrapy genspider <nome> <dom√≠nio>
```

* `<nome>`: define o nome do seu coletor (Spider);
* `<dom√≠nio>`: define o endere√ßo da web (URL) onde ser√° feita a coleta de dados.

Este comando cria um novo coletor (*Spider*) dentro da pasta atual ou na pasta de `spiders` do projeto atual. 
Lembrando que os par√¢metros `<nome>` e `<dom√≠nio>` **s√£o obrigat√≥rios** para que o comando funcione!

#### Exemplo

Entre no diret√≥rio `quotes_scrape_crawler/quotes_scrape_crawler` e digite o comando:

```bash
scrapy genspider citacoes quotes.toscrape.com
```

Essa instru√ß√£o cria um novo coletor (*Spider*) chamando: `citacoes.py`; dentro da pasta `quotes_scrape_crawler/spiders` contendo o seguinte c√≥digo gerado automaticamente:

```python
# -*- coding: utf-8 -*-
import scrapy

class CitacoesSpider(scrapy.Spider):

    name = 'citacoes'
    allowed_domains = ['quotes.toscrape.com']
    start_urls = ['http://quotes.toscrape.com/']

    def parse(self, response):
        pass
```

Observe que o par√¢metro `<nome>` configurou o nome do coletor como `name = 'cronograma_lancamento_nasa'` e o par√¢metro `<dom√≠nio>` definiu os atributos `allowed_domains` e `start_urls` que especificam respectivamente a lista de dom√≠nios permitidos que o coletor pode rastrear e a lista de URLs de onde o coletor (*Spider*) come√ßar√° o processo de rastreamento.

## Extraindo Dados

Ap√≥s configurar seu Rastreador Web (Web Crawler), o primeiro passo que voc√™ precisa fazer para coletar dados em p√°ginas da Web √© estudar a estrutura do documento `HTML` e definir qual ser√° a sua estrat√©gia para obter os dados. No nosso caso queremos coletar as seguintes informa√ß√µes do website [Quotes to Scrape](http://quotes.toscrape.com/): 

* **Texto:** texto da cita√ß√£o;
* **Autor:** nome do autor;
* **Sobre:** endere√ßo web (URL) para a p√°gina dedicada ao autor da cita√ß√£o;
* **tags:** Lista de tags da cita√ß√£o.

![Card de Cita√ß√£o](img/card.png "Cita√ß√£o - Quotes to Scrape")


```html
<div class="quote" itemscope="" itemtype="http://schema.org/CreativeWork">
    
    <span class="text" itemprop="text">
        ‚ÄúThe world as we have created it is a process of our thinking.
        It cannot be changed without changing our thinking.‚Äù
    </span>
    
    <span>by <small class="author" itemprop="author">Albert Einstein</small>
        <a href="/author/Albert-Einstein">(about)</a>
    </span>
    
    <div class="tags">
        Tags:
        <meta class="keywords" itemprop="keywords" content="change,deep-thoughts,thinking,world">
        <a class="tag" href="/tag/change/page/1/">change</a>
        <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>
        <a class="tag" href="/tag/thinking/page/1/">thinking</a>
        <a class="tag" href="/tag/world/page/1/">world</a>
    </div>

</div>
```

* **Texto:** `div.quote/span.text`;
* **Autor:**  `div.quote/small.author`;
* **Sobre:** `div.quote/span/a`;
* **tags:** `div.quote/div.tags/a.tag`.

Ap√≥s fazer este estudo e mapeamento da estrutura do `HTML` da p√°gina Web que ser√° feita a coleta, o pr√≥ximo passo √© programaramos nosso coletor (*Spider*) para visitar cada um destes elementos HTML que guardam nossos dados e fazermos a coleta de dados. Como voc√™ deve imaginar, iremos utilizar os Seletores CSS para acessar cada um dos elementos HTML na √°rvore DOM da p√°gina para pegar nossos dados.

```python
import scrapy

class CitacoesSpider(scrapy.Spider):
    
    name = 'citacoes'                                                   # Nome do coletor (Spider)
    start_urls = [                                                      # Listas de endere√ßos que ser√£o visitados
        'http://quotes.toscrape.com/page/1',
        'http://quotes.toscrape.com/page/2',
    ]

    def parse(self, response):
        for citacao in response.css('div.quote'):                       # Para cada cita√ß√£o da p√°gina web dentro do elemento div.quote 
            yield {
                'texto': citacao.css('span.text::text').get(),          # O m√©todo get() pega a primeira ocorr√™ncia do seletor
                'autor': citacao.css('small.author::text').get(),
                'sobre': citacao.css('span a::attr(href)').get(),
                'tags': citacao.css('div.tags a.tag::text').getall(),   # O m√©todo getall() pega todas as ocorr√™ncias do seletor
            }
```

O c√≥digo acima visita cada um dos endere√ßos na lista de `start_urls`, seleciona todos os *cards* de cita√ß√µes atrav√©s do seletor css `div.quote`, e, visita cada um deles atrav√©s da estrutura de repeti√ß√£o `for`. Dentro do la√ßo `for` √© feita a coleta dos dados usando os seguintes seletores:

* **Texto:** `span.text::text`
* **Autor:**  `small.author::text`
* **Sobre:** `span a::attr(href)`
* **tags:** `div.tags a.tag::text`

Finalmente, execute o comando abaixo para executar o seu c√≥digo, obter os dados e salvar em um arquivo `.json` ou `.csv` na ra√≠z do projeto.

```bash
scrapy crawl citacoes -o cita√ß√µes.json

scrapy crawl citacoes -o cita√ß√µes.csv
```

### Seguindo Links

At√© o momento nosso web crawler coleta apenas dados dos endere√ßos das p√°ginas `http://quotes.toscrape.com/page/1` e `http://quotes.toscrape.com/page/2`, por√©m, √© poss√≠vel visitarmos cada uma das p√°ginas do website para fazer uma coleta completa de dados.

![Bot√£o Pr√≥ximo](img/proxima_pagina_btn.png "Paginador - Bot√£o Pr√≥ximo")

```html
<ul class="pager">
    <li class="next">
        <a href="/page/2/">Next <span aria-hidden="true">‚Üí</span></a>
    </li>
</ul>
```

Para isso precisamos identificar o paginador do website, estudar a estrutura HTML para definir a melhor maneira de selecionar o elemento HTML para pegar o link para a pr√≥xima p√°gina. Em nosso caso, o paginador √© um simples bot√£o pr√≥ximo com o link para a pr√≥xima p√°gina, acess√≠vel atrav√©s do caminho: `ul.pager/li.next/a`, sendo assim, usaremos o seletor `li.next a::attr(href)` para pegar a url da pr√≥xima p√°gina no atributo `href` da tag `<a>`


```python
import scrapy

class CitacoesSpider(scrapy.Spider):
    
    name = 'citacoes'                                       
    start_urls = [                                          
        'http://quotes.toscrape.com/',                                     # Ponto de Partida √önico: P√°gina Inicial
    ]

    def parse(self, response):
        for citacao in response.css('div.quote'):                       
            yield {
                'texto': citacao.css('span.text::text').get(),
                'autor': citacao.css('small.author::text').get(),
                'sobre': citacao.css('span a::attr(href)').get(),
                'tags': citacao.css('div.tags a.tag::text').getall(),   
            }

        proxima_pagina = response.css('li.next a::attr(href)').get()        # Pega o link da pr√≥xima p√°gina no paginador no final da p√°gina web

        if proxima_pagina is not None:                                      # Se o link n√£o for nulo (vazio), ou seja, se existir uma nova p√°gina
            proxima_pagina = response.urljoin(proxima_pagina)               # obt√©m o link da pr√≥xima p√°gina e combina com o endere√ßo atual da URL
            yield scrapy.Request(proxima_pagina, callback=self.parse)       # Chama recursivamente a fun√ß√£o parse() para visitar a nova p√°gina e coletar dados
```

